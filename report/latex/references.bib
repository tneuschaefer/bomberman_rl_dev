Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@incollection{MDP_formal,
abstract = {Publisher Summary Algorithms are specifications of action sequences, similar to recipes for cooking. The description should be concrete enough to cook a tasteful meal. On the other hand, some abstraction is necessary to keep the presentation readable; no one teaches the cook how to dice onions. In presenting algorithms in computer science, the situation is similar. The presentation should be concrete enough to allow analysis and reimplementation, but abstract enough to be ported on different programming languages and machines. The process of problem solving can often be modeled as a search in a state space starting from some given initial state with rules describing how to transform one state into another. They have to be applied over and over again to eventually satisfy some goal condition. In many areas of computer science, heuristics are viewed as practical rules of thumb. In artificial intelligence search, however, heuristics are well-defined mappings of states to numbers. There are different types of search heuristics. This chapter introduces notational background, specific puzzles, and general problem formalisms. It recalls the success story of heuristic search and studies heuristics as efficiently computable lower bounds.},
address = {San Francisco},
author = {Edelkamp, Stefan and Schr{\"{o}}dl, Stefan},
booktitle = {Heuristic Search},
doi = {https://doi.org/10.1016/B978-0-12-372512-7.00001-8},
editor = {Edelkamp, Stefan and Schr{\"{o}}dl, Stefan},
isbn = {978-0-12-372512-7},
keywords = {Markov decision process,Rubik's Cube,STRIPS-type planning,Sokoban,admissible heuristic,asymptotic resource consumption,complexity theory,computability theory,consistent heuristic,production system,route planning,sliding-tile problem,symbolic logic,traveling salesman problem},
pages = {3--46},
publisher = {Morgan Kaufmann},
title = {{Chapter 1 - Introduction}},
url = {https://www.sciencedirect.com/science/article/pii/B9780123725127000018},
year = {2012}
}
@book{BookRL,
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
address = {Cambridge, MA, USA},
author = {Sutton, Richard S and Barto, Andrew G},
isbn = {0262039249},
publisher = {A Bradford Book},
title = {{Reinforcement Learning: An Introduction}},
year = {2018}
}
@article{The2009,
author = {The, E Xpand and Redit, S Aver S C},
file = {:Users/angelinabasova/Library/Application Support/Mendeley Desktop/Downloaded/The, Redit - 2009 - PLAN-BASED RELAXED REWARD SHAPING FOR GOAL-DIRECTED TASKS.pdf:pdf},
pages = {1--17},
title = {{PLAN-BASED RELAXED REWARD SHAPING FOR GOAL-DIRECTED TASKS}},
year = {2009}
}
@article{qlearning,
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/BF00992698},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {http://link.springer.com/10.1007/BF00992698},
volume = {8},
year = {1992}
}
@inproceedings{DynRewardShaping,
abstract = {Potential-based reward shaping can significantly improve the time needed to learn an optimal policy and, in multi-agent systems, the performance of the final joint-policy. It has been proven to not alter the optimal policy of an agent learning alone or the Nash equilibria of multiple agents learning together. However, a limitation of existing proofs is the assumption that the potential of a state does not change dynamically during the learning. This assumption often is broken, especially if the reward-shaping function is generated automatically. In this paper we prove and demonstrate a method of extending potential-based reward shaping to allow dynamic shaping and maintain the guarantees of policy invariance in the single-agent case and consistent Nash equilibria in the multi-agent case.},
address = {Valencia, Spain},
author = {Devlin, Sam and Kudenko, Daniel},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012)},
file = {:Users/angelinabasova/Library/Application Support/Mendeley Desktop/Downloaded/Devlin, Kudenko - 2012 - Dynamic Potential-Based Reward Shaping.pdf:pdf},
isbn = {0981738117},
keywords = {reinforcement learning,reward shaping},
pages = {433--440},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
title = {{Dynamic Potential-Based Reward Shaping}},
year = {2012}
}
@article{Hu2020,
abstract = {Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones.},
archivePrefix = {arXiv},
arxivId = {2011.02669},
author = {Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
eprint = {2011.02669},
file = {:Users/angelinabasova/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2020 - Learning to utilize shaping rewards A new approach of reward shaping(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--22},
title = {{Learning to utilize shaping rewards: A new approach of reward shaping}},
volume = {2020-Decem},
year = {2020}
}
@book{MDP,
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
address = {Cambridge, MA, USA},
author = {Littman, M.L.},
booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences},
doi = {10.1016/B0-08-043076-7/00614-8},
isbn = {0262039249},
pages = {9240--9242},
publisher = {Elsevier},
title = {{Markov Decision Processes}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B0080430767006148},
year = {2001}
}
@article{Grzes2008,
abstract = {Reinforcement learning, while being a highly popular learning technique for agents and multi-agent systems, has so far encountered difficulties when applying it to more complex domains due to scaling-up problems. This paper focuses on the use of domain knowledge to improve the convergence speed and optimality of various RL techniques. Specifically, we propose the use of high-level STRIPS operator knowledge in reward shaping to focus the search for the optimal policy. Empirical results show that the plan-based reward shaping approach outperforms other RL techniques, including alternative manual and MDP-based reward shaping when it is used in its basic form. We show that MDP-based reward shaping may fail and successful experiments with STRIPS-based shaping suggest modifications which can overcome encountered problems. The STRIPS-based method we propose allows expressing the same domain knowledge in a different way and the domain expert can choose whether to define an MDP or STRIPS planning task. We also evaluate the robustness of the proposed STRIPS-based technique to errors in the plan knowledge. {\textcopyright} 2008 IEEE.},
author = {Grzes, Marek and Kudenko, Daniel},
doi = {10.1109/IS.2008.4670492},
file = {:Users/angelinabasova/Library/Application Support/Mendeley Desktop/Downloaded/Grzes, Kudenko - 2008 - Plan-based reward shaping for reinforcement learning(2).pdf:pdf},
isbn = {9781424417391},
journal = {2008 4th International IEEE Conference Intelligent Systems, IS 2008},
keywords = {Reinforcement learning,Reward shaping,STRIPS,Symbolic planning},
number = {May 2020},
pages = {1022--1029},
title = {{Plan-based reward shaping for reinforcement learning}},
volume = {3},
year = {2008}
}
@article{Unknown,
abstract = {The truncated eigenvalue equation is derived from the improved U(1) lattice gauge field Hamiltonian. The vacuum wave function in 2 + 1 dimensional U(1) model is computed. The numerical results conform our expectation that the scaling behavior of the vacuum wave function can be greatly improved by improving the lattice Hamiltonian.},
author = {Jiang, Junqin and Luo, Xiangqian and Guo, Shuohong and Liu, Jinming},
file = {:Users/angelinabasova/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 1999 - Policy invariance under reward transformations Theory and application to reward shaping.pdf:pdf},
issn = {02543052},
journal = {Kao Neng Wu Li Yu Ho Wu Li/High Energy Physics and Nuclear Physics},
keywords = {Improved Hamiltonian,Lattice gauge,Vacuum wave function},
number = {12},
pages = {1156--1157},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
volume = {23},
year = {1999}
}
